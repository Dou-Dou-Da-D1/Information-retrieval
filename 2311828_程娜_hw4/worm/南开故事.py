import requests
import os
import pandas as pd
from bs4 import BeautifulSoup
import re
import hashlib


# é…ç½®éƒ¨åˆ†
URLS = [
    f'https://news.nankai.edu.cn/nkrw/system/count//0008000/000000000000/000/000/c0008000000000000000_0000000{i}.shtml'
    for i in range(10,68)
]
URLS.append('https://news.nankai.edu.cn/nkrw/index.shtml')
OUTPUT_DIR = 'pages'
CSV_PATH = 'title2url.csv'

# åˆ›å»ºä¿å­˜HTMLçš„æ–‡ä»¶å¤¹
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# åˆå§‹åŒ–æ ‡é¢˜ä¸URLæ˜ å°„è¡¨
title2url_df = pd.DataFrame(columns=['url', 'filename'])
title2url_df.index.name = 'title'


def crawlIndex(urls):
    """çˆ¬å–ç´¢å¼•é¡µï¼Œæå–æ–°é—»é“¾æ¥å¹¶ä¿å­˜å†…å®¹"""
    cnt = 0
    for url in urls:
        cnt += 1
        print(f"{cnt}/{len(urls)}: Processing Index Page: {url}")
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # ä¸»åŠ¨æ£€æŸ¥HTTPé”™è¯¯

            soup = BeautifulSoup(response.content, 'html.parser')
            # æå–æ–°é—»é“¾æ¥
            links = soup.find_all('a', href=re.compile(r'/system/\d{4}/'))

            for link in links:
                href = link['href']
                if href.startswith('/'):
                    href = f"https://news.nankai.edu.cn{href}"
                text = link.get_text().strip()

                if len(text) > 0 and "index.shtml" not in href:
                    # ç›´æ¥ä¿å­˜HTMLå¹¶è®°å½•æ˜ å°„
                    save_article(href, text)

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Request error for index page {url}: {e}")
        except Exception as e:
            print(f"âš ï¸ Unexpected error processing index page {url}: {e}")


def save_article(url, title):
    """ä¿å­˜å•ç¯‡æ–°é—»çš„HTMLå†…å®¹å¹¶è®°å½•æ ‡é¢˜ä¸URLæ˜ å°„"""
    global title2url_df  # ç¡®ä¿ä½¿ç”¨å…¨å±€DataFrame

    try:
        # è¿‡æ»¤æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        valid_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        # ä½¿ç”¨URLçš„å“ˆå¸Œå€¼ç¡®ä¿å”¯ä¸€æ€§
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        filename = f"{valid_title}_{url_hash}.html"
        filepath = os.path.join(OUTPUT_DIR, filename)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(filepath):
            print(f"  â© Skipping existing file: {filename}")
        else:
            # è·å–æ–‡ç« å†…å®¹
            response = requests.get(url, timeout=15)
            response.raise_for_status()  # ä¸»åŠ¨æ£€æŸ¥HTTPé”™è¯¯

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"  âœ… Saved: {filename}")

        # è®°å½•æ ‡é¢˜ã€URLå’Œæ–‡ä»¶åçš„æ˜ å°„å…³ç³»ï¼ˆæ— è®ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼‰
        title2url_df.loc[valid_title] = {'url': url, 'filename': filename}

    except requests.exceptions.RequestException as e:
        print(f"  âš ï¸ Request error for article {url}: {e}")
    except Exception as e:
        print(f"  âš ï¸ Unexpected error saving article {url}: {e}")


def save_mapping_to_csv():
    """å°†æ ‡é¢˜ä¸URLçš„æ˜ å°„ä¿å­˜åˆ°CSVæ–‡ä»¶"""
    try:
        if not title2url_df.empty:
            title2url_df.to_csv(CSV_PATH, encoding='utf-8-sig')
            print(f"ğŸ“Š Saved {len(title2url_df)} article mappings to {CSV_PATH}")
            print(f"ğŸ” Example mapping: {title2url_df.head(1).to_dict('records')}")
        else:
            print("âš ï¸ No articles were processed. Check the log for errors.")
    except Exception as e:
        print(f"âš ï¸ Error saving CSV: {e}")
        # ä¿å­˜å¤±è´¥æ—¶ï¼Œè¾“å‡ºæ˜ å°„è¡¨å†…å®¹åˆ°æ§åˆ¶å°ä¾›è°ƒè¯•
        if not title2url_df.empty:
            print("ğŸ“‹ Mapping table content:")
            print(title2url_df.head().to_csv(sep='\t', na_rep='nan'))


if __name__ == '__main__':
    try:
        print(f"ğŸš€ Starting crawler. Saving HTML files to '{OUTPUT_DIR}', mapping to '{CSV_PATH}'")
        crawlIndex(URLS)
        save_mapping_to_csv()
        print("âœ… Crawling completed.")
    except KeyboardInterrupt:
        print("\nğŸ›‘ Crawling interrupted by user.")
        save_mapping_to_csv()  # å³ä½¿ä¸­æ–­ä¹Ÿå°è¯•ä¿å­˜å·²æŠ“å–çš„æ•°æ®
    except Exception as e:
        print(f"âš ï¸ Fatal error: {e}")
        save_mapping_to_csv()  # å‘ç”Ÿä¸¥é‡é”™è¯¯æ—¶ä¹Ÿå°è¯•ä¿å­˜